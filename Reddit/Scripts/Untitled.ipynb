{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b60286-081e-46f7-b8ff-7713b8cf83db",
   "metadata": {},
   "source": [
    "---\n",
    "## Reddit Data\n",
    "\n",
    "    In this script I'll explore the data received by Joao on 27th nov 23 and do some analysis for query expansion as it was done for Google Patents\n",
    "    https://www.kaggle.com/code/jyingong/data-cleaning-sentiment-analysis-reddit\n",
    "    https://github.com/LoLei/redditcleaner\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4d6fbf1-d4a2-4ee8-a52b-76c2e1e836b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting redditcleaner\n",
      "  Downloading redditcleaner-1.1.2-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: redditcleaner\n",
      "Successfully installed redditcleaner-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install redditcleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9156a810-b879-4e68-b907-dbd9d535909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/trinidadbosch/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/trinidadbosch/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/trinidadbosch/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #data handling\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import re  #  preprocessing\n",
    "from time import time  # time operations\n",
    "from collections import defaultdict  # word frequency\n",
    "from sklearn.decomposition import PCA #dimension reduction\n",
    "\n",
    "#import spacy  # For preprocessing\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", \n",
    "                    datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "# Import needed packages for preprocessing\n",
    "import re\n",
    "import string                    \n",
    "nltk.download('wordnet')\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#clean reddit text\n",
    "import redditcleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0754dd66-da41-4fc9-8dda-75971ebdef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv(\"data/query_reddit_nov23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30fd9bed-4855-4e4e-b4a8-481e756198da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dataset', 'created_utc', 'retrieved_utc', 'num_comments', 'score',\n",
       "       'upvote_ratio', 'num_crossposts', 'gilded', 'subreddit_subscribers',\n",
       "       'stickied', 'locked', 'quarantine', 'over_18', 'id', 'author',\n",
       "       'author_flair_text', 'link_flair_text', 'subreddit', 'domain', 'title',\n",
       "       'selftext'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d850ea6-5313-469b-bdfe-64aaeba67512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fc6e495-9d91-4a6e-b44c-c5b8a843bd20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Select few columns to work\n",
    "reddit2 = reddit.iloc[:, [0,1,8,13,14,16,17,18,19,20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4161c1a-aa1e-4c45-8225-fd943366dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3dcd0c7-8bc8-460f-881e-5bb3b445c863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset                       0\n",
       "created_utc                   0\n",
       "subreddit_subscribers     92895\n",
       "id                            0\n",
       "author                        0\n",
       "link_flair_text          342921\n",
       "subreddit                   310\n",
       "domain                     3502\n",
       "title                         0\n",
       "selftext                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7b194e1-8088-41b8-a20e-bea6f973d76f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-b7adbb2db0b5>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reddit2['title'] = reddit2['title'].astype(str)\n",
      "<ipython-input-49-b7adbb2db0b5>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reddit2['selftext'] = reddit2['selftext'].astype(str)\n",
      "<ipython-input-49-b7adbb2db0b5>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reddit2['author'] = reddit2['author'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "## columns to string\n",
    "reddit2.dtypes\n",
    "reddit2['title'] = reddit2['title'].astype(str)\n",
    "reddit2['selftext'] = reddit2['selftext'].astype(str)\n",
    "reddit2['author'] = reddit2['author'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e15ac06-52c8-4afd-a53e-9f093422dc1b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-093e26b9115b>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reddit2['selftext'] = reddit2['selftext'].map(redditcleaner.clean)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679681905</td>\n",
       "      <td>351036.0</td>\n",
       "      <td>120syxa</td>\n",
       "      <td>mieplol</td>\n",
       "      <td>Production question</td>\n",
       "      <td>videoediting</td>\n",
       "      <td>self.VideoEditing</td>\n",
       "      <td>Best motion interpolation AI right now?</td>\n",
       "      <td>I saw lots of deep learning software which did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679681964</td>\n",
       "      <td>215904.0</td>\n",
       "      <td>120szzj</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>artificial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A new artificial intelligence search engine wh...</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679682059</td>\n",
       "      <td>98.0</td>\n",
       "      <td>120t1ss</td>\n",
       "      <td>echojobs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>golangjob</td>\n",
       "      <td>echojobs.io</td>\n",
       "      <td>Motional is hiring Principal Motion Planning E...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679682205</td>\n",
       "      <td>35.0</td>\n",
       "      <td>120t4iv</td>\n",
       "      <td>hotentrancetrain</td>\n",
       "      <td>AI trading opprtunities</td>\n",
       "      <td>aitradingopportunity</td>\n",
       "      <td>self.AItradingOpportunity</td>\n",
       "      <td>Machine Learning Basics for Trading and Investing</td>\n",
       "      <td>\\n\\nLet's break down the basics of machine lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679682314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120t6fd</td>\n",
       "      <td>vinaykarthik2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u_vinaykarthik2</td>\n",
       "      <td>self.VinayKarthik2</td>\n",
       "      <td>Intelligent Process Automation Services</td>\n",
       "      <td>are a type of business solution that combines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475725</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679674517</td>\n",
       "      <td>379476.0</td>\n",
       "      <td>120p2v5</td>\n",
       "      <td>perseus_dg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gradschool</td>\n",
       "      <td>self.GradSchool</td>\n",
       "      <td>Confused about choosing a robotics program .</td>\n",
       "      <td>Hello \\nI got an admit into a masters program ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475726</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679675432</td>\n",
       "      <td>2767308.0</td>\n",
       "      <td>120pijw</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Fan Creations</td>\n",
       "      <td>starwars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am obsessed with the Cantina aliens, so I ma...</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475727</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679675459</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120pj2f</td>\n",
       "      <td>n0slandbotmod</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n0sland</td>\n",
       "      <td>news.google.com</td>\n",
       "      <td>Artificial Intelligence is booming – but how w...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475728</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679675459</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120pj2o</td>\n",
       "      <td>n0slandbotmod</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n0sland</td>\n",
       "      <td>news.google.com</td>\n",
       "      <td>ChatGPT and artificial intelligence tools coul...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475729</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679679200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120rmto</td>\n",
       "      <td>no_influence_1781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u_no_influence_1781</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>As a software engineer, what's the best skill ...</td>\n",
       "      <td>As a software engineer, the best skill set to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475730 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset  created_utc  subreddit_subscribers       id  \\\n",
       "0       2023-03   1679681905               351036.0  120syxa   \n",
       "1       2023-03   1679681964               215904.0  120szzj   \n",
       "2       2023-03   1679682059                   98.0  120t1ss   \n",
       "3       2023-03   1679682205                   35.0  120t4iv   \n",
       "4       2023-03   1679682314                    0.0  120t6fd   \n",
       "...         ...          ...                    ...      ...   \n",
       "475725  2023-03   1679674517               379476.0  120p2v5   \n",
       "475726  2023-03   1679675432              2767308.0  120pijw   \n",
       "475727  2023-03   1679675459                    2.0  120pj2f   \n",
       "475728  2023-03   1679675459                    2.0  120pj2o   \n",
       "475729  2023-03   1679679200                    0.0  120rmto   \n",
       "\n",
       "                   author          link_flair_text             subreddit  \\\n",
       "0                 mieplol      Production question          videoediting   \n",
       "1               [deleted]         Machine Learning            artificial   \n",
       "2                echojobs                      NaN             golangjob   \n",
       "3        hotentrancetrain  AI trading opprtunities  aitradingopportunity   \n",
       "4           vinaykarthik2                      NaN       u_vinaykarthik2   \n",
       "...                   ...                      ...                   ...   \n",
       "475725         perseus_dg                      NaN            gradschool   \n",
       "475726          [deleted]            Fan Creations              starwars   \n",
       "475727      n0slandbotmod                      NaN               n0sland   \n",
       "475728      n0slandbotmod                      NaN               n0sland   \n",
       "475729  no_influence_1781                      NaN   u_no_influence_1781   \n",
       "\n",
       "                           domain  \\\n",
       "0               self.VideoEditing   \n",
       "1                             NaN   \n",
       "2                     echojobs.io   \n",
       "3       self.AItradingOpportunity   \n",
       "4              self.VinayKarthik2   \n",
       "...                           ...   \n",
       "475725            self.GradSchool   \n",
       "475726                        NaN   \n",
       "475727            news.google.com   \n",
       "475728            news.google.com   \n",
       "475729                  i.redd.it   \n",
       "\n",
       "                                                    title  \\\n",
       "0                 Best motion interpolation AI right now?   \n",
       "1       A new artificial intelligence search engine wh...   \n",
       "2       Motional is hiring Principal Motion Planning E...   \n",
       "3       Machine Learning Basics for Trading and Investing   \n",
       "4                 Intelligent Process Automation Services   \n",
       "...                                                   ...   \n",
       "475725       Confused about choosing a robotics program .   \n",
       "475726  I am obsessed with the Cantina aliens, so I ma...   \n",
       "475727  Artificial Intelligence is booming – but how w...   \n",
       "475728  ChatGPT and artificial intelligence tools coul...   \n",
       "475729  As a software engineer, what's the best skill ...   \n",
       "\n",
       "                                                 selftext  \n",
       "0       I saw lots of deep learning software which did...  \n",
       "1                                               [removed]  \n",
       "2                                                     nan  \n",
       "3       \\n\\nLet's break down the basics of machine lea...  \n",
       "4        are a type of business solution that combines...  \n",
       "...                                                   ...  \n",
       "475725  Hello \\nI got an admit into a masters program ...  \n",
       "475726                                          [removed]  \n",
       "475727                                                nan  \n",
       "475728                                                nan  \n",
       "475729  As a software engineer, the best skill set to ...  \n",
       "\n",
       "[475730 rows x 10 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply (map) the function on all body column entries\n",
    "reddit2['selftext'] = reddit2['selftext'].map(redditcleaner.clean)\n",
    "reddit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbfb5a42-f60e-4c99-9f25-a398e950629e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         I saw lots of deep learning software which did...\n",
       "1                                                 [removed]\n",
       "2                                                       nan\n",
       "3         \\n\\nLet's break down the basics of machine lea...\n",
       "4          are a type of business solution that combines...\n",
       "                                ...                        \n",
       "475725    Hello \\nI got an admit into a masters program ...\n",
       "475726                                            [removed]\n",
       "475727                                                  nan\n",
       "475728                                                  nan\n",
       "475729    As a software engineer, the best skill set to ...\n",
       "Name: selftext, Length: 475730, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit2.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e38e0b67-fd65-451f-9491-d30de9429895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         I saw lots of deep learning software which did...\n",
       "1                                                 [removed]\n",
       "2                                                       NaN\n",
       "3          \\n\\nLet's break down the basics of machine le...\n",
       "4         [Intelligent Process Automation services](http...\n",
       "                                ...                        \n",
       "475725    Hello \\nI got an admit into a masters program ...\n",
       "475726                                            [removed]\n",
       "475727                                                  NaN\n",
       "475728                                                  NaN\n",
       "475729    As a software engineer, the best skill set to ...\n",
       "Name: selftext, Length: 475730, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86173310-05b2-4498-80f2-98b864b18685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-23fa3aea15a2>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reddit2.drop_duplicates(subset=['title'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "reddit2.drop_duplicates(subset=['title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10cbdd85-20c5-4404-a103-8eff6ab13056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         I saw lots of deep learning software which did...\n",
       "1                                                 [removed]\n",
       "2                                                       NaN\n",
       "3          \\n\\nLet's break down the basics of machine le...\n",
       "4         [Intelligent Process Automation services](http...\n",
       "                                ...                        \n",
       "475725    Hello \\nI got an admit into a masters program ...\n",
       "475726                                            [removed]\n",
       "475727                                                  NaN\n",
       "475728                                                  NaN\n",
       "475729    As a software engineer, the best skill set to ...\n",
       "Name: selftext, Length: 475730, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit2)\n",
    "reddit.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45804d31-d171-42f6-9faa-0bcbd56d8b11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-eb2d3e7c4d02>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reddit2[\"selftext\"] = [item.split(\"\\n\\n\") for item in reddit2.selftext]\n"
     ]
    }
   ],
   "source": [
    "#split sentences by delimited \"\\n\\n\"\n",
    "reddit2[\"selftext\"] = [item.split(\"\\n\\n\") for item in reddit2.selftext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "183aaf44-58ce-4e1b-b6bd-c36035551b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [I saw lots of deep learning software which di...\n",
       "1                                               [[removed]]\n",
       "2                                                     [nan]\n",
       "3         [ \\n\\nLet's break down the basics of machine l...\n",
       "4         [[Intelligent Process Automation services](htt...\n",
       "                                ...                        \n",
       "475722                                                [nan]\n",
       "475725    [Hello \\nI got an admit into a masters program...\n",
       "475726                                          [[removed]]\n",
       "475728                                                [nan]\n",
       "475729    [As a software engineer, the best skill set to...\n",
       "Name: selftext, Length: 332153, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit2.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b591d34a-73bf-48df-b88d-2838940831b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#explode nested list into individual rows \n",
    "reddit2 = reddit2.explode(\"selftext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2440b8d-4555-456f-b67f-9f64b66fa219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace double space with empty string\n",
    "reddit2[\"selftext\"] = reddit2.selftext.str.replace(\"&#x200B;\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5890ac74-a97e-4b71-aa8f-deadabb67e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reddit2.loc[reddit2.selftext.str.contains(\"https\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6d442186-a7eb-4596-95f5-4de052bb5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see table of items with https links and markup links \n",
    "# create a table to see these authors\n",
    "#df_temp_https = reddit2[reddit2.selftext.str.contains(\"https\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25490c31-2941-4ade-8c61-42b4d11659bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to remove both links and markup links \n",
    "# also remove \\' from dont\\'t\n",
    "def remove_https(item):\n",
    "    #remove https links\n",
    "    item_1 = re.sub(r\"[(+*)]\\S*https?:\\S*[(+*)]\", \"\", item)\n",
    "    #remove https links with no brackets\n",
    "    item_2 = re.sub('http://\\S+|https://\\S+', \" \", item_1)\n",
    "    #remove link markups []\n",
    "    #note that this will also remove comment fields with [\"Delete\"] \n",
    "    item_3 = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \" \", item_2)\n",
    "#     #remove \\ in don\\'t\n",
    "#     item_4 = re.sub(\"[\\\"\\']\", \"'\", item_3)\n",
    "    return item_3\n",
    "\n",
    "reddit2[\"selftext\"] = [remove_https(x) for x in reddit2.selftext]\n",
    "## Do the same for titles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2d80677-2433-4d30-af7b-bcb2c7a5ed1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         I saw lots of deep learning software which did...\n",
       "1                                                          \n",
       "2                                                       nan\n",
       "3          \\n\\nLet's break down the basics of machine le...\n",
       "4           are a type of business solution that combine...\n",
       "                                ...                        \n",
       "475722                                                  nan\n",
       "475725    Hello \\nI got an admit into a masters program ...\n",
       "475726                                                     \n",
       "475728                                                  nan\n",
       "475729    As a software engineer, the best skill set to ...\n",
       "Name: selftext, Length: 332153, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit2.selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "43d29b65-2de3-40f7-b970-2ea7e103eebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-94-c4ad84d3f099>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_temp_https[\"selftext\"] = [remove_https(x) for x in df_temp_https.selftext]\n"
     ]
    }
   ],
   "source": [
    "#check the temporary table to see if links/ markuplinks/ \\' \n",
    "#all links has been removed\n",
    "#unecessary comments (highlighted in yellow) can be removed later by filtering out unecessary usernames\n",
    "df_temp_https[\"selftext\"] = [remove_https(x) for x in df_temp_https.selftext]\n",
    "# df_temp.style.apply(lambda x: ['background: lightyellow' if x.username == \"RemindMeBot\" \\\n",
    "                               # or x.author ==\"sneakpeek_bot\" else '' for i in x], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17bc1a0c-68b0-41ba-acbf-d7114dbd7cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reddit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88229794-07c7-4641-a4ce-da42b63f57a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit2[reddit2.author.isna()]\n",
    "\n",
    "# getting the bots authors for analysis of thos comments\n",
    "reddit_bots = reddit2[reddit2['author'].str.contains('bots', case=False, na=False)]\n",
    "\n",
    "# drop bots authors from reddit dataset\n",
    "reddit_work = reddit2[~reddit2['author'].str.contains('bot', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b5846ed-0f81-476a-8bcc-0721fb4ecc20",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679911875</td>\n",
       "      <td>2932.0</td>\n",
       "      <td>123idw3</td>\n",
       "      <td>neurobotsil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zepeto_official</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>AI bot for Zepeto. Cybersista</td>\n",
       "      <td>\\nWe visited the premises of \"Neurobots\", a co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1680184194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>126noi3</td>\n",
       "      <td>neurobotsil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u_neurobotsil</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>Controlled AI Bots for the Gaming Industry</td>\n",
       "      <td>Controlled AI bots are a new direction in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6762</th>\n",
       "      <td>2012-04</td>\n",
       "      <td>1334448709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sa5va</td>\n",
       "      <td>robotsrobotschicken</td>\n",
       "      <td>NaN</td>\n",
       "      <td>machinelearning</td>\n",
       "      <td>self.MachineLearning</td>\n",
       "      <td>Writing a paper on artificial intelligence</td>\n",
       "      <td>So I'm writing a very basic introductory paper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9345</th>\n",
       "      <td>2013-06</td>\n",
       "      <td>1370623510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1fvddx</td>\n",
       "      <td>stupidrobots</td>\n",
       "      <td>Intro</td>\n",
       "      <td>random_acts_of_amazon</td>\n",
       "      <td>self.Random_Acts_Of_Amazon</td>\n",
       "      <td>[INTRO]I've actually already gifted but hello,...</td>\n",
       "      <td>I love gift exchanges.  I love giving things a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10950</th>\n",
       "      <td>2013-12</td>\n",
       "      <td>1388387532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1u01wu</td>\n",
       "      <td>alsorobots</td>\n",
       "      <td>NaN</td>\n",
       "      <td>askreddit</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>Reddit, if you were a super-villain, what woul...</td>\n",
       "      <td>I assume it involves genetics or cloning or po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449977</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>1674827314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10ml8sj</td>\n",
       "      <td>chatbots2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u_chatbots2023</td>\n",
       "      <td>self.chatbots2023</td>\n",
       "      <td>CHATGPT (CHAT.OPENAI) EXPLAINS HOW COUNTRIES C...</td>\n",
       "      <td>\\n\\n\\n### CHATGPT (CHAT.OPENAI) EXPLAINS H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450078</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>1674684786</td>\n",
       "      <td>16702.0</td>\n",
       "      <td>10lbdb3</td>\n",
       "      <td>botsondemand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nocode</td>\n",
       "      <td>self.nocode</td>\n",
       "      <td>Microsoft and OpenAI Partnership: Impact on AI...</td>\n",
       "      <td>Hey everyone, I'm curious to hear your thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450449</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>1674901346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10nby1p</td>\n",
       "      <td>botspice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u_botspice</td>\n",
       "      <td>self.botspice</td>\n",
       "      <td>How Can You Boost Customer Conversion Using Ch...</td>\n",
       "      <td>Every business thrives on customer conversion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460114</th>\n",
       "      <td>2023-02</td>\n",
       "      <td>1676760441</td>\n",
       "      <td>27432.0</td>\n",
       "      <td>115ufp0</td>\n",
       "      <td>robotswanttokillme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lexfridman</td>\n",
       "      <td>theguardian.com</td>\n",
       "      <td>‘I want to destroy whatever I want’: Bing’s AI...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463602</th>\n",
       "      <td>2023-02</td>\n",
       "      <td>1677524210</td>\n",
       "      <td>27430.0</td>\n",
       "      <td>11djmyl</td>\n",
       "      <td>robotswanttokillme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lexfridman</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>Artificial Intelligence: Last Week Tonight wit...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset  created_utc  subreddit_subscribers       id  \\\n",
       "931     2023-03   1679911875                 2932.0  123idw3   \n",
       "3199    2023-03   1680184194                    0.0  126noi3   \n",
       "6762    2012-04   1334448709                    NaN    sa5va   \n",
       "9345    2013-06   1370623510                    NaN   1fvddx   \n",
       "10950   2013-12   1388387532                    NaN   1u01wu   \n",
       "...         ...          ...                    ...      ...   \n",
       "449977  2023-01   1674827314                    0.0  10ml8sj   \n",
       "450078  2023-01   1674684786                16702.0  10lbdb3   \n",
       "450449  2023-01   1674901346                    0.0  10nby1p   \n",
       "460114  2023-02   1676760441                27432.0  115ufp0   \n",
       "463602  2023-02   1677524210                27430.0  11djmyl   \n",
       "\n",
       "                     author link_flair_text              subreddit  \\\n",
       "931             neurobotsil             NaN        zepeto_official   \n",
       "3199            neurobotsil             NaN          u_neurobotsil   \n",
       "6762    robotsrobotschicken             NaN        machinelearning   \n",
       "9345           stupidrobots           Intro  random_acts_of_amazon   \n",
       "10950            alsorobots             NaN              askreddit   \n",
       "...                     ...             ...                    ...   \n",
       "449977         chatbots2023             NaN         u_chatbots2023   \n",
       "450078         botsondemand             NaN                 nocode   \n",
       "450449             botspice             NaN             u_botspice   \n",
       "460114   robotswanttokillme             NaN             lexfridman   \n",
       "463602   robotswanttokillme             NaN             lexfridman   \n",
       "\n",
       "                            domain  \\\n",
       "931                     reddit.com   \n",
       "3199                     i.redd.it   \n",
       "6762          self.MachineLearning   \n",
       "9345    self.Random_Acts_Of_Amazon   \n",
       "10950               self.AskReddit   \n",
       "...                            ...   \n",
       "449977           self.chatbots2023   \n",
       "450078                 self.nocode   \n",
       "450449               self.botspice   \n",
       "460114             theguardian.com   \n",
       "463602                 youtube.com   \n",
       "\n",
       "                                                    title  \\\n",
       "931                         AI bot for Zepeto. Cybersista   \n",
       "3199           Controlled AI Bots for the Gaming Industry   \n",
       "6762           Writing a paper on artificial intelligence   \n",
       "9345    [INTRO]I've actually already gifted but hello,...   \n",
       "10950   Reddit, if you were a super-villain, what woul...   \n",
       "...                                                   ...   \n",
       "449977  CHATGPT (CHAT.OPENAI) EXPLAINS HOW COUNTRIES C...   \n",
       "450078  Microsoft and OpenAI Partnership: Impact on AI...   \n",
       "450449  How Can You Boost Customer Conversion Using Ch...   \n",
       "460114  ‘I want to destroy whatever I want’: Bing’s AI...   \n",
       "463602  Artificial Intelligence: Last Week Tonight wit...   \n",
       "\n",
       "                                                 selftext  \n",
       "931     \\nWe visited the premises of \"Neurobots\", a co...  \n",
       "3199    Controlled AI bots are a new direction in the ...  \n",
       "6762    So I'm writing a very basic introductory paper...  \n",
       "9345    I love gift exchanges.  I love giving things a...  \n",
       "10950   I assume it involves genetics or cloning or po...  \n",
       "...                                                   ...  \n",
       "449977      \\n\\n\\n### CHATGPT (CHAT.OPENAI) EXPLAINS H...  \n",
       "450078  Hey everyone, I'm curious to hear your thought...  \n",
       "450449  Every business thrives on customer conversion ...  \n",
       "460114                                                nan  \n",
       "463602                                                nan  \n",
       "\n",
       "[187 rows x 10 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_bots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d183ac-5735-4316-baf4-8a290ab38212",
   "metadata": {},
   "source": [
    "## Text cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "21effed6-843b-4c80-aee2-79486215c57f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## CHANGE THIS TO A FUNCTION!!!\n",
    "\n",
    "\n",
    "# #ensure that comment attribute is of correct data type\n",
    "# reddit2[\"selftext\"] = reddit2.selftext.astype(\"str\")\n",
    "# reddit2[\"selftext\"] = [item.lower() for item in reddit2.selftext]\n",
    "\n",
    "# #remove apostrophe at the beginning and end of each word (e.g. 'like, 'this, or', this')\n",
    "# reddit2[\"selftext\"] = [re.sub(r\"(\\B'\\b)|(\\b'\\B)\", ' ', item) for item in reddit2.selftext]\n",
    "# reddit2[\"selftext\"] = [re.sub(r'…', ' ', item) for item in reddit2.selftext]\n",
    "# reddit2[\"selftext\"] = [item.replace('\\\\',' ') for item in reddit2.selftext]\n",
    "# reddit2[\"selftext\"] = [item.replace('/',' ') for item in reddit2.selftext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba753cd-647a-4cf1-9211-efe889f430a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74d5cc71-6855-41ee-87df-8de841164e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(comment: str) -> str:\n",
    "    '''\n",
    "    Function takes a comment and removes http, bitly links from it. Afterwards returning the comment string.\n",
    "    '''\n",
    "    \n",
    "    # Removing http links\n",
    "    comment = re.sub(r'http\\S+', '', comment)   \n",
    "    \n",
    "    # Removing bitly links\n",
    "    comment = re.sub(r'bit.ly/\\S+', '', comment)  \n",
    "    \n",
    "    # Removing link mentioning \n",
    "    comment = comment.strip('[link]')   # remove [links]\n",
    "    \n",
    "    # Removing links to pictures\n",
    "    comment = re.sub(r'pic.twitter\\S+','', comment)\n",
    "    \n",
    "    # Returning tweet without link mentions\n",
    "    return comment\n",
    "\n",
    "\n",
    "def remove_audiovisual(comment: str) -> str:\n",
    "    '''\n",
    "    This function takes a tweet and removes videos and audio tags or labels. Afterwards returning the cleaned tweet.\n",
    "    '''\n",
    "    \n",
    "    # Removing 'VIDEO:' from start of a tweet\n",
    "    comment = re.sub('VIDEO:', '', comment)  \n",
    "    \n",
    "    # Removing 'AUDIO:' from start of a tweet\n",
    "    comment = re.sub('AUDIO:', '', comment)  \n",
    "    \n",
    "    # Returning tweet without videos and audio tags or labels\n",
    "    return comment\n",
    "\n",
    "\n",
    "def nlp_preprocess(comments: pd.Series) -> list:\n",
    "    ''' \n",
    "    This NLP function cleans tweets so that they resemble sentences used in the daily language, thus for further analysis no tweet specific trained models are necessray to use. \n",
    "    '''\n",
    "    \n",
    "    # Creating an empty list to fill with preprocessed tweets\n",
    "    comments_preprocessed = []\n",
    "\n",
    "    for comment in comments:\n",
    "        # Removing urls, hashtags, mentions, reserved words (RT, FAV), emojis, and smileys\n",
    "        #comment = pre.clean(comment)\n",
    "        \n",
    "        # Applying predefined functions for tweet cleaning\n",
    "        comment = remove_links(comment)\n",
    "        comment = remove_audiovisual(comment)\n",
    "        \n",
    "        # Removing number mentionings\n",
    "        comment = re.sub('([0-9]+)', '', comment)  \n",
    "        \n",
    "        # Removing words containing numbers\n",
    "        comment = re.sub('\\w*\\d\\w*', '', comment)  \n",
    "        \n",
    "        # Removing line break \n",
    "        comment = re.sub('\\n', '', comment)\n",
    "        \n",
    "        # Strip punctuation special to tweets, but uncommon in natural language sentences\n",
    "        punctuation = '\"$%&\\'*+:;/()<=>[\\\\]^_`{|}~•@£'\n",
    "        comment = re.sub('[' + punctuation + ']+', ' ', comment)  \n",
    "        \n",
    "        # Removing double/multiple spacing\n",
    "        comment = re.sub('\\s+', ' ', comment)   \n",
    "        \n",
    "        # Normalize characters, means concretly --> todaaaaay = today\n",
    "        comment = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', comment) \n",
    "        \n",
    "        # Appending preprocessed tweet to the list tweets_preprocessed. If tweet now empty append NaN\n",
    "        if comment != '' and comment != ' ':\n",
    "            comments_preprocessed.append(comment)\n",
    "        else:\n",
    "            comments_preprocessed.append('NaN') \n",
    "    \n",
    "    # Returning list of processed tweets\n",
    "    return comments_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15aa1b7-cfa4-4ce8-a3d2-b9736714ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying nlp_preprocess() function to the tweets column to preprocess the data for further analysis\n",
    "#reddit_bots['selftext'] = nlp_preprocess(reddit_bots['selftext'])  \n",
    "reddit2['selftext'] = reddit2['selftext'].apply(nlp_preprocess)\n",
    "#reddit_work['selftext'] = nlp_preprocess(reddit_work['selftext']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "127e178f-3f7d-4fba-8f40-f1203749f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"None\")\n",
    "\n",
    "def preprocessing(text):\n",
    "    if isinstance(text, str) and text.lower() != \"none\":\n",
    "        words = word_tokenize(text.lower())  # Tokenize the text and convert to lowercase\n",
    "        filtered_words = [word for word in words if word not in stop_words and not any(char.isdigit() for char in word) and word not in string.punctuation]  # Remove stopwords, numbers, and punctuation\n",
    "        return filtered_words\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "116d5984-c518-4608-bbef-44f3414b14ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-9690f20d94b3>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reddit_bots['selftext'] = reddit_bots['selftext'].apply(preprocessing)\n"
     ]
    }
   ],
   "source": [
    "reddit_bots['selftext'] = reddit_bots['selftext'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1067644-ab12-4e88-b1de-b0a21866e9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1679911875</td>\n",
       "      <td>2932.0</td>\n",
       "      <td>123idw3</td>\n",
       "      <td>neurobotsil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zepeto_official</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>AI bot for Zepeto. Cybersista</td>\n",
       "      <td>[\\nwe, visited, premises, neurobots, company, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>2023-03</td>\n",
       "      <td>1680184194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>126noi3</td>\n",
       "      <td>neurobotsil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u_neurobotsil</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>Controlled AI Bots for the Gaming Industry</td>\n",
       "      <td>[controlled, ai, bots, new, direction, field, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6762</th>\n",
       "      <td>2012-04</td>\n",
       "      <td>1334448709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sa5va</td>\n",
       "      <td>robotsrobotschicken</td>\n",
       "      <td>NaN</td>\n",
       "      <td>machinelearning</td>\n",
       "      <td>self.MachineLearning</td>\n",
       "      <td>Writing a paper on artificial intelligence</td>\n",
       "      <td>[writing, basic, introductory, paper, artifici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9345</th>\n",
       "      <td>2013-06</td>\n",
       "      <td>1370623510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1fvddx</td>\n",
       "      <td>stupidrobots</td>\n",
       "      <td>Intro</td>\n",
       "      <td>random_acts_of_amazon</td>\n",
       "      <td>self.Random_Acts_Of_Amazon</td>\n",
       "      <td>[INTRO]I've actually already gifted but hello,...</td>\n",
       "      <td>[love, gift, exchanges, love, giving, things, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10950</th>\n",
       "      <td>2013-12</td>\n",
       "      <td>1388387532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1u01wu</td>\n",
       "      <td>alsorobots</td>\n",
       "      <td>NaN</td>\n",
       "      <td>askreddit</td>\n",
       "      <td>self.AskReddit</td>\n",
       "      <td>Reddit, if you were a super-villain, what woul...</td>\n",
       "      <td>[assume, involves, genetics, cloning, possibly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449977</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>1674827314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10ml8sj</td>\n",
       "      <td>chatbots2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u_chatbots2023</td>\n",
       "      <td>self.chatbots2023</td>\n",
       "      <td>CHATGPT (CHAT.OPENAI) EXPLAINS HOW COUNTRIES C...</td>\n",
       "      <td>[\\n\\n\\n, chatgpt, chat.openai, explains, count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450078</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>1674684786</td>\n",
       "      <td>16702.0</td>\n",
       "      <td>10lbdb3</td>\n",
       "      <td>botsondemand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nocode</td>\n",
       "      <td>self.nocode</td>\n",
       "      <td>Microsoft and OpenAI Partnership: Impact on AI...</td>\n",
       "      <td>[hey, everyone, curious, hear, thoughts, recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450449</th>\n",
       "      <td>2023-01</td>\n",
       "      <td>1674901346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10nby1p</td>\n",
       "      <td>botspice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u_botspice</td>\n",
       "      <td>self.botspice</td>\n",
       "      <td>How Can You Boost Customer Conversion Using Ch...</td>\n",
       "      <td>[every, business, thrives, customer, conversio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460114</th>\n",
       "      <td>2023-02</td>\n",
       "      <td>1676760441</td>\n",
       "      <td>27432.0</td>\n",
       "      <td>115ufp0</td>\n",
       "      <td>robotswanttokillme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lexfridman</td>\n",
       "      <td>theguardian.com</td>\n",
       "      <td>‘I want to destroy whatever I want’: Bing’s AI...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463602</th>\n",
       "      <td>2023-02</td>\n",
       "      <td>1677524210</td>\n",
       "      <td>27430.0</td>\n",
       "      <td>11djmyl</td>\n",
       "      <td>robotswanttokillme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lexfridman</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>Artificial Intelligence: Last Week Tonight wit...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset  created_utc  subreddit_subscribers       id  \\\n",
       "931     2023-03   1679911875                 2932.0  123idw3   \n",
       "3199    2023-03   1680184194                    0.0  126noi3   \n",
       "6762    2012-04   1334448709                    NaN    sa5va   \n",
       "9345    2013-06   1370623510                    NaN   1fvddx   \n",
       "10950   2013-12   1388387532                    NaN   1u01wu   \n",
       "...         ...          ...                    ...      ...   \n",
       "449977  2023-01   1674827314                    0.0  10ml8sj   \n",
       "450078  2023-01   1674684786                16702.0  10lbdb3   \n",
       "450449  2023-01   1674901346                    0.0  10nby1p   \n",
       "460114  2023-02   1676760441                27432.0  115ufp0   \n",
       "463602  2023-02   1677524210                27430.0  11djmyl   \n",
       "\n",
       "                     author link_flair_text              subreddit  \\\n",
       "931             neurobotsil             NaN        zepeto_official   \n",
       "3199            neurobotsil             NaN          u_neurobotsil   \n",
       "6762    robotsrobotschicken             NaN        machinelearning   \n",
       "9345           stupidrobots           Intro  random_acts_of_amazon   \n",
       "10950            alsorobots             NaN              askreddit   \n",
       "...                     ...             ...                    ...   \n",
       "449977         chatbots2023             NaN         u_chatbots2023   \n",
       "450078         botsondemand             NaN                 nocode   \n",
       "450449             botspice             NaN             u_botspice   \n",
       "460114   robotswanttokillme             NaN             lexfridman   \n",
       "463602   robotswanttokillme             NaN             lexfridman   \n",
       "\n",
       "                            domain  \\\n",
       "931                     reddit.com   \n",
       "3199                     i.redd.it   \n",
       "6762          self.MachineLearning   \n",
       "9345    self.Random_Acts_Of_Amazon   \n",
       "10950               self.AskReddit   \n",
       "...                            ...   \n",
       "449977           self.chatbots2023   \n",
       "450078                 self.nocode   \n",
       "450449               self.botspice   \n",
       "460114             theguardian.com   \n",
       "463602                 youtube.com   \n",
       "\n",
       "                                                    title  \\\n",
       "931                         AI bot for Zepeto. Cybersista   \n",
       "3199           Controlled AI Bots for the Gaming Industry   \n",
       "6762           Writing a paper on artificial intelligence   \n",
       "9345    [INTRO]I've actually already gifted but hello,...   \n",
       "10950   Reddit, if you were a super-villain, what woul...   \n",
       "...                                                   ...   \n",
       "449977  CHATGPT (CHAT.OPENAI) EXPLAINS HOW COUNTRIES C...   \n",
       "450078  Microsoft and OpenAI Partnership: Impact on AI...   \n",
       "450449  How Can You Boost Customer Conversion Using Ch...   \n",
       "460114  ‘I want to destroy whatever I want’: Bing’s AI...   \n",
       "463602  Artificial Intelligence: Last Week Tonight wit...   \n",
       "\n",
       "                                                 selftext  \n",
       "931     [\\nwe, visited, premises, neurobots, company, ...  \n",
       "3199    [controlled, ai, bots, new, direction, field, ...  \n",
       "6762    [writing, basic, introductory, paper, artifici...  \n",
       "9345    [love, gift, exchanges, love, giving, things, ...  \n",
       "10950   [assume, involves, genetics, cloning, possibly...  \n",
       "...                                                   ...  \n",
       "449977  [\\n\\n\\n, chatgpt, chat.openai, explains, count...  \n",
       "450078  [hey, everyone, curious, hear, thoughts, recen...  \n",
       "450449  [every, business, thrives, customer, conversio...  \n",
       "460114                                                 []  \n",
       "463602                                                 []  \n",
       "\n",
       "[187 rows x 10 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43aa87-2f93-4e79-8451-139742e66ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
