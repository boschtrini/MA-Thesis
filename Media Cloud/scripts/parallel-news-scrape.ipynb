{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5598c4a1-3428-40cd-b26d-3f8a5cce307a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## News Scraping by parallelization\n",
    "    In this scrip I built a function to scrape news using the library \"newspaper\"  https://github.com/codelucas/newspaper/ and I apply it over the urls downloaded in Media-Cloud. \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0fa436-536b-4523-909f-cc1afc6be8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor ## parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e1d902c-8779-45a6-a4b7-89854031f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_articles_new(ids, urls):\n",
    "    article_data = {\n",
    "        'ID': [],\n",
    "        'URL': [],\n",
    "        'Body': [],\n",
    "        'MetaData': [],\n",
    "        'Authors': [],\n",
    "        'Date': [],\n",
    "        'Title': [],\n",
    "        'Tags': [],\n",
    "        'MetaKeywords': [],\n",
    "        'Summary': [],\n",
    "        'Error': [],  # New column for error messages\n",
    "    }\n",
    "\n",
    "    errors = []  # List to store error messages\n",
    "    \n",
    "    start_time = time.time()  # Record the start time\n",
    "    \n",
    "    for id_, url in zip(ids, urls):\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            \n",
    "            # Append the parsed article to the dictionary\n",
    "            article_data['ID'].append(id_)\n",
    "            article_data['URL'].append(url)\n",
    "            article_data['Body'].append(article.text)\n",
    "            article_data['MetaData'].append(article.meta_description)\n",
    "            article_data['Authors'].append(article.authors)\n",
    "            article_data['Date'].append(article.publish_date)\n",
    "            article_data['Title'].append(article.title)\n",
    "            article_data['Tags'].append(article.tags)\n",
    "            article_data['MetaKeywords'].append(article.meta_keywords)\n",
    "            \n",
    "            article.nlp()\n",
    "            article_data['Summary'].append(article.summary)\n",
    "            \n",
    "            # No error, so add an empty string to the 'Error' column\n",
    "            article_data['Error'].append('')\n",
    "            \n",
    "            # Introduce a sleep to avoid being blocked\n",
    "            time.sleep(3)  # Sleep for 3 seconds\n",
    "            \n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle URL-related exceptions\n",
    "            article_data['Error'].append(str(e))\n",
    "            errors.append(str(e))  # Append error to the list\n",
    "            \n",
    "            # Append 'ID' and 'URL'\n",
    "            article_data['ID'].append(id_)\n",
    "            article_data['URL'].append(url)\n",
    "            \n",
    "            # Append placeholders for other columns\n",
    "            for column in ['Body', 'MetaData', 'Authors', 'Date', 'Title', 'Tags', 'MetaKeywords', 'Summary']:\n",
    "                article_data[column].append('NA')\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Handle other exceptions (e.g., parsing errors)\n",
    "            article_data['Error'].append(str(e))\n",
    "            errors.append(str(e))  # Append error to the list\n",
    "            \n",
    "            # Append 'ID' and 'URL'\n",
    "            article_data['ID'].append(id_)\n",
    "            article_data['URL'].append(url)\n",
    "            \n",
    "            # Append placeholders for other columns\n",
    "            for column in ['Body', 'MetaData', 'Authors', 'Date', 'Title', 'Tags', 'MetaKeywords', 'Summary']:\n",
    "                article_data[column].append('NA')\n",
    "    \n",
    "    end_time = time.time()  # Record the end time\n",
    "    \n",
    "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "    \n",
    "    print(f\"Time taken to fetch and parse articles: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return pd.DataFrame(article_data), errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d90a56ef-7a29-4bd4-9900-e66f52f40dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_url = pd.read_pickle(\"/Users/trinidadbosch/Desktop/SEDS/Tesis/Data/MA-Thesis/Media Cloud/Data/media_urls.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8d0c44cf-a02b-433b-aeda-1c1d0154ca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ap_syndicated', 'collect_date', 'feeds', 'guid', 'language',\n",
       "       'media_id', 'media_name', 'media_url', 'metadata',\n",
       "       'processed_stories_id', 'publish_date', 'stories_id', 'story_tags',\n",
       "       'title', 'url', 'word_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_url.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "75ce4b1f-112d-4222-a33d-0024ad178763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2021-02-01 03:11:18\n",
       "1         2021-02-01 13:28:31\n",
       "2         2021-02-01 13:17:36\n",
       "3         2021-02-01 02:05:19\n",
       "4         2021-02-01 14:07:26\n",
       "                 ...         \n",
       "192989    2023-11-01 17:21:46\n",
       "192990    2023-10-30 19:46:44\n",
       "192991    2023-11-01 18:00:00\n",
       "192992    2023-10-31 18:00:00\n",
       "192993    2023-11-01 09:00:00\n",
       "Name: publish_date, Length: 192994, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_url.publish_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93d02e-a945-409f-a9b5-72823f5a4461",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Apply scraping function in paralell\n",
    "    This process took ~20hrs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcac43cf-13d2-42b3-a972-878e0b70033c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fetch and parse articles: 3784.77 seconds\n",
      "Time taken to fetch and parse articles: 4202.48 seconds\n",
      "Time taken to fetch and parse articles: 5815.62 seconds\n",
      "Time taken to fetch and parse articles: 5924.41 seconds\n",
      "Time taken to fetch and parse articles: 6227.64 seconds\n",
      "Time taken to fetch and parse articles: 6367.67 seconds\n",
      "Time taken to fetch and parse articles: 6432.34 seconds\n",
      "Time taken to fetch and parse articles: 6619.20 seconds\n",
      "Time taken to fetch and parse articles: 5454.46 seconds\n",
      "Time taken to fetch and parse articles: 3411.00 seconds\n",
      "Time taken to fetch and parse articles: 3689.31 seconds\n",
      "Time taken to fetch and parse articles: 7391.17 seconds\n",
      "Time taken to fetch and parse articles: 6684.15 seconds\n",
      "Time taken to fetch and parse articles: 6778.27 seconds\n",
      "Time taken to fetch and parse articles: 6661.36 seconds\n",
      "Time taken to fetch and parse articles: 6498.51 seconds\n",
      "Time taken to fetch and parse articles: 4893.06 seconds\n",
      "Time taken to fetch and parse articles: 4665.54 seconds\n",
      "Time taken to fetch and parse articles: 6454.67 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /opt/anaconda3/lib/python3.8/site-packages/jieba/dict.txt ...\n",
      "Dumping model to file cache /var/folders/vk/yr_rw4912x38st74np992k1r0000gn/T/jieba.cache\n",
      "Loading model cost 3.11118221282959 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fetch and parse articles: 4385.26 seconds\n",
      "Time taken to fetch and parse articles: 3492.28 seconds\n",
      "Time taken to fetch and parse articles: 4274.16 seconds\n",
      "Time taken to fetch and parse articles: 5542.20 seconds\n",
      "Time taken to fetch and parse articles: 3882.53 seconds\n",
      "Time taken to fetch and parse articles: 6708.33 seconds\n",
      "Time taken to fetch and parse articles: 6249.68 seconds\n",
      "Time taken to fetch and parse articles: 6659.23 seconds\n",
      "Time taken to fetch and parse articles: 6318.34 seconds\n",
      "Time taken to fetch and parse articles: 6664.81 seconds\n",
      "Time taken to fetch and parse articles: 3367.04 seconds\n",
      "Time taken to fetch and parse articles: 3321.14 seconds\n",
      "Time taken to fetch and parse articles: 5339.08 seconds\n",
      "Time taken to fetch and parse articles: 6585.64 seconds\n",
      "Time taken to fetch and parse articles: 3748.51 seconds\n",
      "Time taken to fetch and parse articles: 3830.97 seconds\n",
      "Time taken to fetch and parse articles: 3487.06 seconds\n",
      "Time taken to fetch and parse articles: 4997.63 seconds\n",
      "Time taken to fetch and parse articles: 5578.42 seconds\n",
      "Time taken to fetch and parse articles: 6461.92 seconds\n",
      "Time taken to fetch and parse articles: 6385.87 seconds\n",
      "Time taken to fetch and parse articles: 6158.90 seconds\n",
      "Time taken to fetch and parse articles: 6273.97 seconds\n",
      "Time taken to fetch and parse articles: 6336.87 seconds\n",
      "Time taken to fetch and parse articles: 6431.28 seconds\n",
      "Time taken to fetch and parse articles: 4522.36 seconds\n",
      "Time taken to fetch and parse articles: 3345.08 seconds\n",
      "Time taken to fetch and parse articles: 4038.89 seconds\n",
      "Time taken to fetch and parse articles: 4181.36 seconds\n",
      "Time taken to fetch and parse articles: 6477.29 seconds\n",
      "Time taken to fetch and parse articles: 3449.04 seconds\n",
      "Time taken to fetch and parse articles: 6089.17 seconds\n",
      "Time taken to fetch and parse articles: 6098.63 seconds\n",
      "Time taken to fetch and parse articles: 4907.51 seconds\n",
      "Time taken to fetch and parse articles: 5899.24 seconds\n",
      "Time taken to fetch and parse articles: 5911.39 seconds\n",
      "Time taken to fetch and parse articles: 5773.43 seconds\n",
      "Time taken to fetch and parse articles: 5798.18 seconds\n",
      "Time taken to fetch and parse articles: 3143.47 seconds\n",
      "Time taken to fetch and parse articles: 4292.90 seconds\n",
      "Time taken to fetch and parse articles: 5930.22 seconds\n",
      "Time taken to fetch and parse articles: 3273.16 seconds\n",
      "Time taken to fetch and parse articles: 6032.40 seconds\n",
      "Time taken to fetch and parse articles: 6148.36 seconds\n",
      "Time taken to fetch and parse articles: 4181.10 seconds\n",
      "Time taken to fetch and parse articles: 4967.12 seconds\n",
      "Time taken to fetch and parse articles: 5299.22 seconds\n",
      "Time taken to fetch and parse articles: 5496.21 seconds\n",
      "Time taken to fetch and parse articles: 5928.38 seconds\n",
      "Time taken to fetch and parse articles: 5686.14 seconds\n",
      "Time taken to fetch and parse articles: 5949.23 seconds\n",
      "Time taken to fetch and parse articles: 5889.52 seconds\n",
      "Time taken to fetch and parse articles: 5985.87 seconds\n",
      "Time taken to fetch and parse articles: 6182.58 seconds\n",
      "Time taken to fetch and parse articles: 6592.47 seconds\n",
      "Time taken to fetch and parse articles: 4886.92 seconds\n",
      "Time taken to fetch and parse articles: 5345.16 seconds\n",
      "Time taken to fetch and parse articles: 6763.22 seconds\n",
      "Time taken to fetch and parse articles: 6034.83 seconds\n",
      "Time taken to fetch and parse articles: 4794.72 seconds\n",
      "Time taken to fetch and parse articles: 6669.45 seconds\n",
      "Time taken to fetch and parse articles: 5433.71 seconds\n",
      "Time taken to fetch and parse articles: 3937.39 seconds\n",
      "Time taken to fetch and parse articles: 3435.61 seconds\n",
      "Time taken to fetch and parse articles: 3795.85 seconds\n",
      "Time taken to fetch and parse articles: 5420.98 seconds\n",
      "Time taken to fetch and parse articles: 5568.46 seconds\n",
      "Time taken to fetch and parse articles: 5831.76 seconds\n",
      "Time taken to fetch and parse articles: 5989.59 seconds\n",
      "Time taken to fetch and parse articles: 5904.36 seconds\n",
      "Time taken to fetch and parse articles: 6097.56 seconds\n",
      "Time taken to fetch and parse articles: 5977.86 seconds\n",
      "Time taken to fetch and parse articles: 5912.74 seconds\n",
      "Time taken to fetch and parse articles: 6138.56 seconds\n",
      "Time taken to fetch and parse articles: 6248.41 seconds\n",
      "Time taken to fetch and parse articles: 328.82 seconds\n",
      "Time taken to fetch and parse articles: 6193.96 seconds\n",
      "Time taken to fetch and parse articles: 6200.27 seconds\n",
      "Time taken to fetch and parse articles: 5521.15 seconds\n",
      "Time taken to fetch and parse articles: 6096.40 seconds\n",
      "Time taken to fetch and parse articles: 6339.69 seconds\n",
      "Time taken to fetch and parse articles: 6181.60 seconds\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of chunks\n",
    "num_chunks = 100\n",
    "\n",
    "# Calculate the size of each chunk\n",
    "chunk_size = len(pkl_url) // num_chunks\n",
    "\n",
    "# Split the DataFrame into chunks\n",
    "df_chunks = [pkl_url.iloc[i:i + chunk_size] for i in range(0, len(pkl_url), chunk_size)]\n",
    "\n",
    "# Function to apply in parallel\n",
    "def process_chunk(chunk):\n",
    "    chunk_ids = chunk['stories_id'].tolist()\n",
    "    chunk_urls = chunk['url'].tolist()\n",
    "    \n",
    "    return fetch_articles_new(chunk_ids, chunk_urls)\n",
    "\n",
    "# Apply the function in parallel using ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_chunk, df_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03e4327a-c231-4963-8d37-12a5b2c43cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Body</th>\n",
       "      <th>MetaData</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>MetaKeywords</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1838925040</td>\n",
       "      <td>https://news.mongabay.com/2021/02/eye-in-the-s...</td>\n",
       "      <td>An interdisciplinary team of zoologists and co...</td>\n",
       "      <td>Environmental science and conservation news</td>\n",
       "      <td>[Terna Gyuse]</td>\n",
       "      <td>2021-02-01 08:11:18+00:00</td>\n",
       "      <td>Eye in the Sky: Tech makes satellite imagery i...</td>\n",
       "      <td>{Wildlife, Animals, Jim Tan, Endangered Specie...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Satellite surveying still offers many advantag...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1839017422</td>\n",
       "      <td>https://www.cnbc.com/2021/02/01/amazon-alphabe...</td>\n",
       "      <td>Ali Ghodsi, co-founder and CEO of Databricks I...</td>\n",
       "      <td>Amazon is getting involved in the start-up, Da...</td>\n",
       "      <td>[Jordan Novet]</td>\n",
       "      <td>2021-02-01 00:00:00</td>\n",
       "      <td>Amazon, Alphabet and Salesforce are all invest...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Start-up, Venture capital, Microsoft Corp, Sa...</td>\n",
       "      <td>Ali Ghodsi, co-founder and CEO of Databricks I...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1839009079</td>\n",
       "      <td>http://feeds.benzinga.com/~r/benzinga/~3/JWduJ...</td>\n",
       "      <td>One of the hottest names in the investing worl...</td>\n",
       "      <td>One of the hottest names in the investing worl...</td>\n",
       "      <td>[Chris Katje]</td>\n",
       "      <td>None</td>\n",
       "      <td>15 Big Ideas In 'Disruptive Innovation' Accord...</td>\n",
       "      <td>{Government, Digital Wallets, Healthcare, Poli...</td>\n",
       "      <td>[]</td>\n",
       "      <td>One of the hottest names in the investing worl...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1838767296</td>\n",
       "      <td>https://eurweb.com/2021/01/31/black-creators-a...</td>\n",
       "      <td>*It’s day four at the 2021 Sundance Film Festi...</td>\n",
       "      <td></td>\n",
       "      <td>[Olivia T.]</td>\n",
       "      <td>2021-01-31 00:00:00</td>\n",
       "      <td>Black Creators At Sundance 2021</td>\n",
       "      <td>{white wedding, sundance, Sophia Nahli Allison...</td>\n",
       "      <td>[]</td>\n",
       "      <td>*It’s day four at the 2021 Sundance Film Festi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1839090241</td>\n",
       "      <td>http://feeds.benzinga.com/~r/benzinga/~3/y-hmu...</td>\n",
       "      <td>Loading... Loading...\\n\\nBill.com BILL shares ...</td>\n",
       "      <td>Bill.com (NYSE: BILL) shares are trading highe...</td>\n",
       "      <td>[Tanzeel Akhtar]</td>\n",
       "      <td>None</td>\n",
       "      <td>Why Bill.com's Stock Is Trading Higher Today -...</td>\n",
       "      <td>{Government, Healthcare, Politics, Regulations...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Loading... Loading...Bill.com BILL shares are ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192989</th>\n",
       "      <td>2816117787</td>\n",
       "      <td>https://arstechnica.com/?p=1980413</td>\n",
       "      <td>On Wednesday, the UK hosted an AI Safety Summi...</td>\n",
       "      <td>\"Bletchley Declaration\" sums up first day of U...</td>\n",
       "      <td>[Benj Edwards]</td>\n",
       "      <td>2023-11-01 21:21:46+00:00</td>\n",
       "      <td>“Catastrophic” AI harms among warnings in decl...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>The event included the signing of \"The Bletchl...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192990</th>\n",
       "      <td>2816156060</td>\n",
       "      <td>https://www.cnbc.com/2023/10/31/stock-markets-...</td>\n",
       "      <td>Traders work on the floor of the New York Stoc...</td>\n",
       "      <td>After a rough week amid accelerating inflation...</td>\n",
       "      <td>[Yeo Boon Ping]</td>\n",
       "      <td>2023-10-31 00:00:00</td>\n",
       "      <td>CNBC Daily Open: Markets’ bounce may be short-...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Technology, Autos, Morgan Stanley, Meta Platf...</td>\n",
       "      <td>This report is from today's CNBC Daily Open, o...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192991</th>\n",
       "      <td>2818282751</td>\n",
       "      <td>https://doakio.com/blog/missteps-in-blockchain...</td>\n",
       "      <td>Introduction\\n\\nIn the world of blockchain tec...</td>\n",
       "      <td></td>\n",
       "      <td>[Rachele Augusto, Var Molongui_Authorship_Fron...</td>\n",
       "      <td>2023-11-01 22:00:00+00:00</td>\n",
       "      <td>Missteps in Blockchain Documentation: A Review</td>\n",
       "      <td>{Uncategorized}</td>\n",
       "      <td>[]</td>\n",
       "      <td>The Importance of Accurate Blockchain Document...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192992</th>\n",
       "      <td>2818282763</td>\n",
       "      <td>https://doakio.com/blog/why-jargon-isnt-always...</td>\n",
       "      <td>Introduction\\n\\nWelcome to the world of techni...</td>\n",
       "      <td></td>\n",
       "      <td>[Rachele Augusto, Var Molongui_Authorship_Fron...</td>\n",
       "      <td>2023-10-31 22:00:00+00:00</td>\n",
       "      <td>Why Jargon Isn’t Always the Enemy: Technical v...</td>\n",
       "      <td>{Uncategorized}</td>\n",
       "      <td>[]</td>\n",
       "      <td>The Pitfalls of Over-Simplifying Technical Ter...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192993</th>\n",
       "      <td>2818282758</td>\n",
       "      <td>https://doakio.com/blog/%ef%bf%bctranslating-t...</td>\n",
       "      <td>Introduction\\n\\nIn today's fast-paced technolo...</td>\n",
       "      <td></td>\n",
       "      <td>[Rachele Augusto, Var Molongui_Authorship_Fron...</td>\n",
       "      <td>2023-11-01 13:00:00+00:00</td>\n",
       "      <td>Translating Tech: Bridging the Gap Between Cod...</td>\n",
       "      <td>{Uncategorized}</td>\n",
       "      <td>[]</td>\n",
       "      <td>Technical documentation often includes complex...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192994 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                                URL  \\\n",
       "0       1838925040  https://news.mongabay.com/2021/02/eye-in-the-s...   \n",
       "1       1839017422  https://www.cnbc.com/2021/02/01/amazon-alphabe...   \n",
       "2       1839009079  http://feeds.benzinga.com/~r/benzinga/~3/JWduJ...   \n",
       "3       1838767296  https://eurweb.com/2021/01/31/black-creators-a...   \n",
       "4       1839090241  http://feeds.benzinga.com/~r/benzinga/~3/y-hmu...   \n",
       "...            ...                                                ...   \n",
       "192989  2816117787                 https://arstechnica.com/?p=1980413   \n",
       "192990  2816156060  https://www.cnbc.com/2023/10/31/stock-markets-...   \n",
       "192991  2818282751  https://doakio.com/blog/missteps-in-blockchain...   \n",
       "192992  2818282763  https://doakio.com/blog/why-jargon-isnt-always...   \n",
       "192993  2818282758  https://doakio.com/blog/%ef%bf%bctranslating-t...   \n",
       "\n",
       "                                                     Body  \\\n",
       "0       An interdisciplinary team of zoologists and co...   \n",
       "1       Ali Ghodsi, co-founder and CEO of Databricks I...   \n",
       "2       One of the hottest names in the investing worl...   \n",
       "3       *It’s day four at the 2021 Sundance Film Festi...   \n",
       "4       Loading... Loading...\\n\\nBill.com BILL shares ...   \n",
       "...                                                   ...   \n",
       "192989  On Wednesday, the UK hosted an AI Safety Summi...   \n",
       "192990  Traders work on the floor of the New York Stoc...   \n",
       "192991  Introduction\\n\\nIn the world of blockchain tec...   \n",
       "192992  Introduction\\n\\nWelcome to the world of techni...   \n",
       "192993  Introduction\\n\\nIn today's fast-paced technolo...   \n",
       "\n",
       "                                                 MetaData  \\\n",
       "0             Environmental science and conservation news   \n",
       "1       Amazon is getting involved in the start-up, Da...   \n",
       "2       One of the hottest names in the investing worl...   \n",
       "3                                                           \n",
       "4       Bill.com (NYSE: BILL) shares are trading highe...   \n",
       "...                                                   ...   \n",
       "192989  \"Bletchley Declaration\" sums up first day of U...   \n",
       "192990  After a rough week amid accelerating inflation...   \n",
       "192991                                                      \n",
       "192992                                                      \n",
       "192993                                                      \n",
       "\n",
       "                                                  Authors  \\\n",
       "0                                           [Terna Gyuse]   \n",
       "1                                          [Jordan Novet]   \n",
       "2                                           [Chris Katje]   \n",
       "3                                             [Olivia T.]   \n",
       "4                                        [Tanzeel Akhtar]   \n",
       "...                                                   ...   \n",
       "192989                                     [Benj Edwards]   \n",
       "192990                                    [Yeo Boon Ping]   \n",
       "192991  [Rachele Augusto, Var Molongui_Authorship_Fron...   \n",
       "192992  [Rachele Augusto, Var Molongui_Authorship_Fron...   \n",
       "192993  [Rachele Augusto, Var Molongui_Authorship_Fron...   \n",
       "\n",
       "                             Date  \\\n",
       "0       2021-02-01 08:11:18+00:00   \n",
       "1             2021-02-01 00:00:00   \n",
       "2                            None   \n",
       "3             2021-01-31 00:00:00   \n",
       "4                            None   \n",
       "...                           ...   \n",
       "192989  2023-11-01 21:21:46+00:00   \n",
       "192990        2023-10-31 00:00:00   \n",
       "192991  2023-11-01 22:00:00+00:00   \n",
       "192992  2023-10-31 22:00:00+00:00   \n",
       "192993  2023-11-01 13:00:00+00:00   \n",
       "\n",
       "                                                    Title  \\\n",
       "0       Eye in the Sky: Tech makes satellite imagery i...   \n",
       "1       Amazon, Alphabet and Salesforce are all invest...   \n",
       "2       15 Big Ideas In 'Disruptive Innovation' Accord...   \n",
       "3                         Black Creators At Sundance 2021   \n",
       "4       Why Bill.com's Stock Is Trading Higher Today -...   \n",
       "...                                                   ...   \n",
       "192989  “Catastrophic” AI harms among warnings in decl...   \n",
       "192990  CNBC Daily Open: Markets’ bounce may be short-...   \n",
       "192991     Missteps in Blockchain Documentation: A Review   \n",
       "192992  Why Jargon Isn’t Always the Enemy: Technical v...   \n",
       "192993  Translating Tech: Bridging the Gap Between Cod...   \n",
       "\n",
       "                                                     Tags  \\\n",
       "0       {Wildlife, Animals, Jim Tan, Endangered Specie...   \n",
       "1                                                      {}   \n",
       "2       {Government, Digital Wallets, Healthcare, Poli...   \n",
       "3       {white wedding, sundance, Sophia Nahli Allison...   \n",
       "4       {Government, Healthcare, Politics, Regulations...   \n",
       "...                                                   ...   \n",
       "192989                                                 {}   \n",
       "192990                                                 {}   \n",
       "192991                                    {Uncategorized}   \n",
       "192992                                    {Uncategorized}   \n",
       "192993                                    {Uncategorized}   \n",
       "\n",
       "                                             MetaKeywords  \\\n",
       "0                                                      []   \n",
       "1       [Start-up, Venture capital, Microsoft Corp, Sa...   \n",
       "2                                                      []   \n",
       "3                                                      []   \n",
       "4                                                      []   \n",
       "...                                                   ...   \n",
       "192989                                                 []   \n",
       "192990  [Technology, Autos, Morgan Stanley, Meta Platf...   \n",
       "192991                                                 []   \n",
       "192992                                                 []   \n",
       "192993                                                 []   \n",
       "\n",
       "                                                  Summary Error  \n",
       "0       Satellite surveying still offers many advantag...        \n",
       "1       Ali Ghodsi, co-founder and CEO of Databricks I...        \n",
       "2       One of the hottest names in the investing worl...        \n",
       "3       *It’s day four at the 2021 Sundance Film Festi...        \n",
       "4       Loading... Loading...Bill.com BILL shares are ...        \n",
       "...                                                   ...   ...  \n",
       "192989  The event included the signing of \"The Bletchl...        \n",
       "192990  This report is from today's CNBC Daily Open, o...        \n",
       "192991  The Importance of Accurate Blockchain Document...        \n",
       "192992  The Pitfalls of Over-Simplifying Technical Ter...        \n",
       "192993  Technical documentation often includes complex...        \n",
       "\n",
       "[192994 rows x 11 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract DataFrames from the tuple results\n",
    "result_dataframes = [result[0] for result in results]\n",
    "\n",
    "# Concatenate the DataFrames into a single DataFrame\n",
    "final_result = pd.concat(result_dataframes, ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dcd2483-169b-4234-8eb6-87f38a25c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "final_result.to_csv('/Users/trinidadbosch/Desktop/SEDS/Tesis/Data/MA-Thesis/Media Cloud/Data/scraped_news.csv', index=False)\n",
    "\n",
    "# Save to pickle\n",
    "final_result.to_pickle('/Users/trinidadbosch/Desktop/SEDS/Tesis/Data/MA-Thesis/Media Cloud/Data/scraped_news.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c69aaac0-0817-4106-b51e-833c7a02b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-empty cells in the specified column\n",
    "error_rows = final_result[final_result['Body'] == 'NA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4c57411d-02fb-411a-8de6-a9f7f5d9f047",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-ffa61fbe873c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merror_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'error_rows' is not defined"
     ]
    }
   ],
   "source": [
    "error_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afa952-052c-4be9-8f27-f5a3514aa38f",
   "metadata": {},
   "source": [
    "---\n",
    "## Scraping with BS\n",
    "    Effort to get those urls that threw error with the previous function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2668abaa-71aa-4e37-a4b8-f17d8fd2e403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdde7dab-38a3-4ff9-beaa-b5766e19436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.seattletimes.com/business/stellantis-foxconn-team-up-to-make-cars-more-connected/?utm_source=RSS&utm_medium=Referral&utm_campaign=RSS_all',\n",
    " 'https://www.thestreet.com/press-releases/u-s-air-force-awards-booz-allen-950m-idiq-contract-15412250',\n",
    " 'https://abcnews.go.com/International/wireStory/british-political-candidate-artificial-intelligence-draw-election-manifesto-101487427']\n",
    "ids = ['1934724599', '1697865428','2717045621']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36bbff97-f1c2-4478-bc5c-069612cdba5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1934724599,\n",
       " 1697865428,\n",
       " 2717045621,\n",
       " 1834599827,\n",
       " 2028472808,\n",
       " 2596037950,\n",
       " 1764666806,\n",
       " 1503351474,\n",
       " 802145235,\n",
       " 956377683]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8803b9f3-dbf0-4ca5-9524-c0c1543acd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs_scrape(urls, ids_):\n",
    "    article_data = {\n",
    "        'ID': [],\n",
    "        'URL': [],\n",
    "        'Body': [],\n",
    "        'Title': [],\n",
    "        'Error': [],\n",
    "    }\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for url, id_ in zip(urls, ids_):  # Use zip to iterate over both lists simultaneously\n",
    "        try:\n",
    "            # Adding a delay between requests to avoid being blocked\n",
    "            time.sleep(5)\n",
    "            \n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title = (soup.find('title')).get_text()\n",
    "            body = soup.find_all('p')\n",
    "\n",
    "            article_data['Title'].append(title)\n",
    "\n",
    "            body_text = \"\"\n",
    "            for text in body:\n",
    "                body_text += text.get_text() + \"\\n\"\n",
    "\n",
    "            article_data['Body'].append(body_text)\n",
    "            article_data['ID'].append(id_)\n",
    "            article_data['URL'].append(url)\n",
    "            article_data['Error'].append(None)  # No error for this entry\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            errors.append(f\"Error processing URL {url}: {str(e)}\")\n",
    "            article_data['Title'].append(None)\n",
    "            article_data['Body'].append(None)\n",
    "            article_data['ID'].append(id_)\n",
    "            article_data['URL'].append(url)\n",
    "            article_data['Error'].append(str(e))\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"Time taken to fetch and parse articles: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    if errors:\n",
    "        print(\"Errors:\")\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "\n",
    "    return pd.DataFrame(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54554621-0177-44f5-9efa-606a919afd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fetch and parse articles: 50.04 seconds\n",
      "Errors:Time taken to fetch and parse articles: 50.05 seconds\n",
      "Errors:\n",
      "Error processing URL h: Invalid URL 'h': No schema supplied. Perhaps you meant http://h?\n",
      "Error processing URL t: Invalid URL 't': No schema supplied. Perhaps you meant http://t?\n",
      "Error processing URL t: Invalid URL 't': No schema supplied. Perhaps you meant http://t?Time taken to fetch and parse articles: 50.05 seconds\n",
      "\n",
      "Errors:\n",
      "Error processing URL h: Invalid URL 'h': No schema supplied. Perhaps you meant http://h?\n",
      "Error processing URL t: Invalid URL 't': No schema supplied. Perhaps you meant http://t?\n",
      "Error processing URL t: Invalid URL 't': No schema supplied. Perhaps you meant http://t?\n",
      "Error processing URL p: Invalid URL 'p': No schema supplied. Perhaps you meant http://p?\n",
      "Error processing URL s: Invalid URL 's': No schema supplied. Perhaps you meant http://s?\n",
      "Error processing URL h: Invalid URL 'h': No schema supplied. Perhaps you meant http://h?\n",
      "Error processing URL t: Invalid URL 't': No schema supplied. Perhaps you meant http://t?\n",
      "Error processing URL t: Invalid URL 't': No schema supplied. Perhaps you meant http://t?\n",
      "\n",
      "Error processing URL :: No connection adapters were found for ':'\n",
      "Error processing URL /: Invalid URL '/': No schema supplied. Perhaps you meant http:///?\n",
      "Error processing URL /: Invalid URL '/': No schema supplied. Perhaps you meant http:///?\n",
      "Error processing URL a: Invalid URL 'a': No schema supplied. Perhaps you meant http://a?\n",
      "Error processing URL b: Invalid URL 'b': No schema supplied. Perhaps you meant http://b?\n",
      "Error processing URL p: Invalid URL 'p': No schema supplied. Perhaps you meant http://p?Error processing URL p: Invalid URL 'p': No schema supplied. Perhaps you meant http://p?\n",
      "Error processing URL s: Invalid URL 's': No schema supplied. Perhaps you meant http://s?\n",
      "Error processing URL :: No connection adapters were found for ':'\n",
      "Error processing URL /: Invalid URL '/': No schema supplied. Perhaps you meant http:///?\n",
      "Error processing URL /: Invalid URL '/': No schema supplied. Perhaps you meant http:///?\n",
      "Error processing URL w: Invalid URL 'w': No schema supplied. Perhaps you meant http://w?\n",
      "Error processing URL w: Invalid URL 'w': No schema supplied. Perhaps you meant http://w?\n",
      "\n",
      "Error processing URL s: Invalid URL 's': No schema supplied. Perhaps you meant http://s?\n",
      "Error processing URL :: No connection adapters were found for ':'\n",
      "Error processing URL /: Invalid URL '/': No schema supplied. Perhaps you meant http:///?\n",
      "Error processing URL /: Invalid URL '/': No schema supplied. Perhaps you meant http:///?\n",
      "Error processing URL w: Invalid URL 'w': No schema supplied. Perhaps you meant http://w?\n",
      "Error processing URL w: Invalid URL 'w': No schema supplied. Perhaps you meant http://w?\n",
      "    0    1     2      3      4\n",
      "0  ID  URL  Body  Title  Error\n",
      "1  ID  URL  Body  Title  Error\n",
      "2  ID  URL  Body  Title  Error\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "# Example usage:\n",
    "urls = [\n",
    "    'https://www.seattletimes.com/business/stellantis-foxconn-team-up-to-make-cars-more-connected/?utm_source=RSS&utm_medium=Referral&utm_campaign=RSS_all',\n",
    "    'https://www.thestreet.com/press-releases/u-s-air-force-awards-booz-allen-950m-idiq-contract-15412250',\n",
    "    'https://abcnews.go.com/International/wireStory/british-political-candidate-artificial-intelligence-draw-election-manifesto-101487427'\n",
    "]\n",
    "ids = ['1934724599', '1697865428', '2717045621']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc7918b-1b39-4193-b675-1cc90245c8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>URL</td>\n",
       "      <td>Body</td>\n",
       "      <td>Title</td>\n",
       "      <td>Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>URL</td>\n",
       "      <td>Body</td>\n",
       "      <td>Title</td>\n",
       "      <td>Error</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID</td>\n",
       "      <td>URL</td>\n",
       "      <td>Body</td>\n",
       "      <td>Title</td>\n",
       "      <td>Error</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1     2      3      4\n",
       "0  ID  URL  Body  Title  Error\n",
       "1  ID  URL  Body  Title  Error\n",
       "2  ID  URL  Body  Title  Error"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee823e3c-2a67-474a-bcc7-2ce5503da9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('/Users/trinidadbosch/Desktop/SEDS/Tesis/Data/MA-Thesis/Media Cloud/Data/scraped_news.csv')\n",
    "urls_raw = pd.read_pickle('/Users/trinidadbosch/Desktop/SEDS/Tesis/Data/MA-Thesis/Media Cloud/Data/media_urls.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d77b3526-05ef-4599-8bf8-f86cb437ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_body_rows = news[news['Body'].isna()]\n",
    "rows_work = nan_body_rows.drop_duplicates(subset='URL')\n",
    "\n",
    "# Sample 100 rows\n",
    "sampled_rows = rows_work.sample(n=10, random_state=42)  # Set a random_state for reproducibility\n",
    "\n",
    "# Extract 'Body' and 'ID' columns from the sampled DataFrame\n",
    "sampled_urls = sampled_rows['URL'].to_list()\n",
    "sampled_ids = sampled_rows['ID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9014101c-eed7-4cad-a3fd-4da35df81fb6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b0ceffc84aca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs_scrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-706750a37b44>\u001b[0m in \u001b[0;36mbs_scrape\u001b[0;34m(urls, ids_)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Raise an HTTPError for bad responses (4xx or 5xx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_df = bs_scrape(urls, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c9c5724-27e4-42cc-99d2-ba56b8800a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1636cca00713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Using ThreadPoolExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs_scrape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Convert the list of dictionaries to a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-46b7aff2236d>\u001b[0m in \u001b[0;36mbs_scrape\u001b[0;34m(urls, ids_)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Use zip to iterate over both lists simultaneously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Adding a delay between requests to avoid being blocked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Using ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(bs_scrape, sampled_urls, sampled_ids))\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "result_df = pd.DataFrame.from_records(results)\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f44fc1-ce43-44b8-a951-1122f370b644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
